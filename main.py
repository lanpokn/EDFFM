import argparse
import yaml
import torch
import os
import numpy as np
import time

from src.epoch_iteration_dataset import create_epoch_iteration_dataloaders
from src.model import EventDenoisingMamba # ç¡®è®¤å¯¼å…¥çš„æ˜¯ä¿®æ­£åçš„æ¨¡å‹
from src.trainer import Trainer
from src.evaluate import Evaluator

def analyze_events_detailed(events: np.ndarray, name: str) -> dict:
    """è¯¦ç»†åˆ†æäº‹ä»¶çš„èŒƒå›´å’Œç»Ÿè®¡ä¿¡æ¯"""
    if len(events) == 0:
        print(f"  ğŸ“Š {name}: EMPTY (0 events)")
        return {'count': 0, 'x_range': None, 'y_range': None, 't_range': None}
    
    x_min, x_max = events[:, 0].min(), events[:, 0].max()
    y_min, y_max = events[:, 1].min(), events[:, 1].max() 
    t_min, t_max = events[:, 2].min(), events[:, 2].max()
    duration_ms = (t_max - t_min) / 1000.0
    
    pos_events = np.sum(events[:, 3] > 0)
    neg_events = np.sum(events[:, 3] < 0)
    
    print(f"  ğŸ“Š {name}: {len(events)} events")
    print(f"    - X range: [{x_min:.1f}, {x_max:.1f}] (span: {x_max-x_min:.1f})")
    print(f"    - Y range: [{y_min:.1f}, {y_max:.1f}] (span: {y_max-y_min:.1f})")
    print(f"    - T range: [{t_min:.0f}, {t_max:.0f}] Î¼s (duration: {duration_ms:.1f}ms)")
    print(f"    - Polarity: {pos_events} positive, {neg_events} negative")
    
    return {
        'count': len(events), 'x_range': (x_min, x_max), 'y_range': (y_min, y_max),
        't_range': (t_min, t_max), 'duration_ms': duration_ms,
        'pos_events': pos_events, 'neg_events': neg_events
    }

def analyze_epoch_iteration_details(train_loader):
    """åˆ†æEpoch-Iterationæ¶æ„çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\nğŸ” è¯¦ç»†åˆ†æEpoch-Iterationæ•°æ®æµ")
    print("=" * 60)
    
    # è·å–æ•°æ®é›†å®ä¾‹
    if hasattr(train_loader, 'dataset'):
        dataset = train_loader.dataset
        
        # æ‰‹åŠ¨ç”Ÿæˆepochæ•°æ®æ¥è·å–è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ”„ ç”Ÿæˆæ–°çš„Epochæ•°æ®...")
        epoch_start = time.time()
        
        # ç”ŸæˆèƒŒæ™¯äº‹ä»¶
        print(f"\nğŸ“– Step 1: ç”ŸæˆèƒŒæ™¯äº‹ä»¶...")
        background_events = dataset._generate_background_events()
        bg_stats = analyze_events_detailed(background_events, "Background Events")
        
        # ç”Ÿæˆç‚«å…‰äº‹ä»¶
        print(f"\nâœ¨ Step 2: ç”Ÿæˆç‚«å…‰äº‹ä»¶...")
        try:
            flare_events_raw = dataset._generate_flare_events()
            if len(flare_events_raw) > 0:
                flare_events = dataset._format_flare_events(flare_events_raw)
            else:
                flare_events = np.empty((0, 4))
            flare_stats = analyze_events_detailed(flare_events, "Flare Events")
        except Exception as e:
            print(f"    âŒ ç‚«å…‰ç”Ÿæˆå¤±è´¥: {e}")
            flare_events = np.empty((0, 4))
            flare_stats = {'count': 0}
        
        # åˆå¹¶äº‹ä»¶
        print(f"\nğŸ”— Step 3: åˆå¹¶äº‹ä»¶...")
        merged_events, merged_labels = dataset._merge_and_sort_events(background_events, flare_events)
        merged_stats = analyze_events_detailed(merged_events, "Merged Events")
        
        # ç‰¹å¾æå–
        print(f"\nğŸ§  Step 4: ç‰¹å¾æå–...")
        feature_start = time.time()
        if len(merged_events) > 0:
            long_features = dataset.feature_extractor.process_sequence(merged_events)
            dataset.long_feature_sequence = long_features
            dataset.long_labels = merged_labels
            dataset.num_iterations = max(1, len(long_features) - dataset.sequence_length + 1)
        else:
            dataset.long_feature_sequence = np.zeros((1, 11), dtype=np.float32)
            dataset.long_labels = np.zeros(1, dtype=np.int64)
            dataset.num_iterations = 1
        
        feature_time = time.time() - feature_start
        epoch_time = time.time() - epoch_start
        
        print(f"  âœ… ç‰¹å¾æå–å®Œæˆ: {feature_time:.3f}s")
        print(f"  ğŸ“Š ç‰¹å¾åºåˆ—å½¢çŠ¶: {dataset.long_feature_sequence.shape}")
        print(f"  ğŸ“Š å¯ç”¨è¿­ä»£æ•°: {dataset.num_iterations}")
        print(f"  ğŸ“Š æ€»Epochæ—¶é—´: {epoch_time:.3f}s")
        
        # åˆ†æiterations
        print(f"\nğŸ¯ Step 5: åˆ†æIterationè¿ç»­æ€§...")
        analyze_iterations(dataset, num_iterations=min(10, dataset.num_iterations))
        
        return True
    else:
        print("  âŒ æ— æ³•è·å–æ•°æ®é›†å®ä¾‹è¿›è¡Œè¯¦ç»†åˆ†æ")
        return False

def analyze_iterations(dataset, num_iterations=10):
    """åˆ†æiterationçš„è¿ç»­æ€§"""
    print(f"\nğŸ“Š åˆ†æå‰{num_iterations}ä¸ªIterations:")
    print("-" * 50)
    
    for i in range(num_iterations):
        try:
            features, labels = dataset[i]
            
            # è½¬æ¢ä¸ºnumpy
            if isinstance(features, torch.Tensor):
                features_np = features.numpy()
                labels_np = labels.numpy()
            else:
                features_np = features
                labels_np = labels
            
            bg_count = np.sum(labels_np == 0)
            flare_count = np.sum(labels_np == 1)
            
            # æ£€æŸ¥æ ‡ç­¾æ®µæ•°ï¼ˆè¿ç»­æ€§æŒ‡æ ‡ï¼‰
            label_changes = np.diff(labels_np.astype(int))
            num_segments = np.sum(np.abs(label_changes)) + 1
            
            print(f"  Iter {i:2d}: {len(features_np):3d} events "
                  f"(BG:{bg_count:3d}, FL:{flare_count:3d}) "
                  f"| {num_segments} segments")
            
            # æ£€æŸ¥è¿ç»­iterationä¹‹é—´çš„è¿ç»­æ€§
            if i > 0:
                prev_features, prev_labels = dataset[i-1]
                if isinstance(prev_features, torch.Tensor):
                    prev_features = prev_features.numpy()
                    prev_labels = prev_labels.numpy()
                
                # æ£€æŸ¥æ»‘åŠ¨çª—å£é‡å éƒ¨åˆ†
                if len(prev_features) == dataset.sequence_length and len(features_np) == dataset.sequence_length:
                    overlap_prev = prev_features[1:]  # å‰ä¸€ä¸ªçš„ååŠéƒ¨åˆ†
                    overlap_curr = features_np[:-1]   # å½“å‰çš„å‰åŠéƒ¨åˆ†
                    
                    if overlap_prev.shape == overlap_curr.shape:
                        max_diff = np.max(np.abs(overlap_prev - overlap_curr))
                        if max_diff < 1e-6:
                            continuity = "âœ…è¿ç»­"
                        else:
                            continuity = f"âŒä¸è¿ç»­({max_diff:.6f})"
                        print(f"        è¿ç»­æ€§æ£€æŸ¥: {continuity}")
        
        except Exception as e:
            print(f"  Iter {i:2d}: âŒ åˆ†æå¤±è´¥: {e}")
    
    print(f"\nâ±ï¸ æ»‘åŠ¨çª—å£è¿ç»­æ€§æ€»ç»“:")
    print(f"  - æ¯ä¸ªiterationåº”è¯¥æœ‰{dataset.sequence_length}ä¸ªäº‹ä»¶")
    print(f"  - ç›¸é‚»iterationåº”è¯¥æœ‰{dataset.sequence_length-1}ä¸ªé‡å äº‹ä»¶")
    print(f"  - æ ‡ç­¾æ®µæ•°è¶Šå°‘è¡¨ç¤ºäº‹ä»¶è¶Šè¿ç»­")

def main(config):
    """
    Main function to run the training and evaluation pipeline.
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Debug mode setup
    if config.get('debug_mode', False):
        output_dir = os.path.join("output", "debug_visualizations")
        os.makedirs(output_dir, exist_ok=True)
        config['debug_output_dir'] = output_dir
        print(f"ğŸš¨ DEBUG MODE: Saving visualizations to {output_dir}")
        print(f"ğŸš¨ DEBUG MODE: Will run limited iterations for debugging")

    # 1. åˆ›å»ºæ•°æ®é›†åŠ è½½å™¨ (ç»Ÿä¸€ä½¿ç”¨Epoch-Iterationæ¶æ„)
    print("ğŸ”„ Using Epoch-Iteration architecture (å…ˆå®Œæ•´åºåˆ—ç‰¹å¾æå–ï¼Œå†æ»‘åŠ¨çª—å£)")
    train_loader, val_loader, test_loader = create_epoch_iteration_dataloaders(config)
    
    # ğŸ” è¯¦ç»†åˆ†æEpoch-Iterationæ•°æ®æµ
    print("\n" + "="*80)
    print("ğŸ” EPOCH-ITERATION è¯¦ç»†æ•°æ®æµåˆ†æ")
    print("="*80)
    analyze_epoch_iteration_details(train_loader)
    print("="*80)

    # 2. åˆå§‹åŒ–æ¨¡å‹ (å…³é”®ä¿®æ­£)
    # ç°åœ¨æˆ‘ä»¬å°†ç‰¹å¾æå–å™¨å’ŒMambaæ¨¡å‹çš„é…ç½®åˆ†å¼€ä¼ é€’
    model = EventDenoisingMamba(config).to(device)
    
    print(f"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    # 3. æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œ
    if config['run']['mode'] == 'analyze':
        print("\nğŸ¯ Analysiså®Œæˆ! é€€å‡ºç¨‹åº (ä¸è¿›è¡Œè®­ç»ƒ)")
        print("è¯¦ç»†çš„Epoch-Iterationåˆ†ææŠ¥å‘Šå·²æ˜¾ç¤ºåœ¨ä¸Šæ–¹ã€‚")
        return
    elif config['run']['mode'] == 'train':
        trainer = Trainer(model, train_loader, val_loader, config, device)
        trainer.train()
    elif config['run']['mode'] == 'evaluate':
        evaluator = Evaluator(model, test_loader, config, device)
        model.load_state_dict(torch.load(config['evaluation']['checkpoint_path']))
        print(f"Loaded checkpoint from: {config['evaluation']['checkpoint_path']}")
        evaluator.evaluate()
    else:
        raise ValueError(f"Unknown mode: {config['run']['mode']}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train and evaluate the EventMamba-FX model.")
    parser.add_argument('--config', type=str, default='configs/config.yaml', help="Path to the YAML configuration file.")
    parser.add_argument('--debug', action='store_true', help="Enable debug mode to save flare image sequences and event visualizations.")
    
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    # Enable debug mode if --debug flag is set
    if args.debug:
        config['debug_mode'] = True
        # Limit iterations for debug mode
        config['training']['max_epochs'] = 1
        config['training']['max_samples_debug'] = 8  # Only process a few samples
        # Debug event visualization parameters (multiple temporal resolutions)
        config['debug_event_subdivisions'] = [0.5, 1, 2, 4]  # Multiple subdivision strategies

    main(config)