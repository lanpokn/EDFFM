# src/unified_dataset.py
import os
import h5py
import time
import glob
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

# å‡è®¾æ‚¨åŸæ¥çš„æ•°æ®ç”Ÿæˆé€»è¾‘å°è£…åœ¨ 'src.epoch_iteration_dataset' ä¸­
from src.epoch_iteration_dataset import EpochIterationDataset as GenerativeBackend

class UnifiedSequenceDataset(Dataset):
    """
    ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œæ”¯æŒä¸¤ç§æ“ä½œæ¨¡å¼ï¼š
    1. 'generate': å®æ—¶ç”Ÿæˆæ•°æ®å¹¶è‡ªåŠ¨å­˜æ¡£ã€‚
    2. 'load': ä»ç£ç›˜åŠ è½½é¢„å…ˆå­˜æ¡£çš„æ•°æ®ã€‚
    """
    def __init__(self, config: dict, split: str):
        self.config = config
        self.split = split
        self.mode = config['data_pipeline']['mode'] # 'generate' or 'load'
        
        data_config = config['data_pipeline']
        self.h5_archive_dir = os.path.join(data_config['h5_archive_path'], split)
        os.makedirs(self.h5_archive_dir, exist_ok=True)

        if self.mode == 'generate':
            self.generator = GenerativeBackend(config, split)
            self.num_sequences_per_epoch = config['training' if split == 'train' else 'evaluation']['num_long_sequences_per_epoch']
            print(f"UnifiedDataset ({split}) in 'generate' mode. Will generate {self.num_sequences_per_epoch} sequences per epoch.")
        
        elif self.mode == 'load':
            self.file_list = sorted(glob.glob(os.path.join(self.h5_archive_dir, '*.h5')))
            if not self.file_list:
                raise FileNotFoundError(f"No .h5 files found in {self.h5_archive_dir}. Please run in 'generate' mode first.")
            self.num_sequences_per_epoch = len(self.file_list)
            print(f"UnifiedDataset ({split}) in 'load' mode. Found {self.num_sequences_per_epoch} pre-generated files.")
        
        else:
            raise ValueError(f"Unknown mode in data_pipeline: {self.mode}")

    def __len__(self):
        return self.num_sequences_per_epoch

    def __getitem__(self, idx):
        if self.mode == 'generate':
            # 1. å®æ—¶ç”Ÿæˆä¸€ä¸ªé•¿åºåˆ—
            features, labels = self.generator._generate_one_long_sequence()
            
            # 2. è‡ªåŠ¨å­˜æ¡£åˆ°ç£ç›˜
            # ä¸ºäº†é˜²æ­¢æ–‡ä»¶åé‡å¤ï¼Œå¯ä»¥ä½¿ç”¨æ—¶é—´æˆ³å’Œç´¢å¼•ç»“åˆ
            timestamp = int(time.time() * 1000)
            filename = f'sequence_{timestamp}_{idx:05d}.h5'
            file_path = os.path.join(self.h5_archive_dir, filename)
            
            try:
                with h5py.File(file_path, 'w') as hf:
                    hf.create_dataset('features', data=features)
                    hf.create_dataset('labels', data=labels)
                print(f"ğŸ’¾ Archived sequence to: {filename} (shape: {features.shape})")
            except Exception as e:
                print(f"Warning: Failed to save archive file {file_path}. Error: {e}")

            # 3. è½¬æ¢ä¸ºTensorå¹¶è¿”å›
            return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)

        elif self.mode == 'load':
            # ç›´æ¥ä»ç£ç›˜è¯»å–æ–‡ä»¶
            file_path = self.file_list[idx]
            with h5py.File(file_path, 'r') as hf:
                features = hf['features'][:]
                labels = hf['labels'][:]
            
            filename = os.path.basename(file_path)
            print(f"ğŸ“‚ Loaded sequence from: {filename} (shape: {features.shape})")
            return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)

def create_unified_dataloaders(config):
    """æ ¹æ®é…ç½®åˆ›å»ºç»Ÿä¸€çš„Dataloader"""
    train_dataset = UnifiedSequenceDataset(config, split='train')
    val_dataset = UnifiedSequenceDataset(config, split='val')
    
    # åœ¨'load'æ¨¡å¼ä¸‹ï¼Œshuffle=Trueæ˜¯å®‰å…¨çš„ï¼Œå¹¶ä¸”å¯ä»¥å¹¶è¡ŒåŠ è½½
    # åœ¨'generate'æ¨¡å¼ä¸‹ï¼Œshuffleæ— æ„ä¹‰ï¼Œnum_workersåº”ä¸º0
    shuffle = (config['data_pipeline']['mode'] == 'load')
    num_workers = config['data_pipeline'].get('num_workers', 0) if shuffle else 0
    
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=shuffle, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers) # éªŒè¯é›†é€šå¸¸ä¸shuffle

    print(f"ğŸ“Š Created unified dataloaders in '{config['data_pipeline']['mode']}' mode")
    print(f"  - Train: {len(train_loader)} sequences per epoch")
    print(f"  - Val: {len(val_loader)} sequences per epoch")
    print(f"  - Shuffle: {shuffle}, Workers: {num_workers}")

    return train_loader, val_loader