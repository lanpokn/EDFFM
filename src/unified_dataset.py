# src/unified_dataset.py
import os
import h5py
import time
import glob
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

# å‡è®¾æ‚¨åŸæ¥çš„æ•°æ®ç”Ÿæˆé€»è¾‘å°è£…åœ¨ 'src.epoch_iteration_dataset' ä¸­
from src.epoch_iteration_dataset import EpochIterationDataset as GenerativeBackend

class UnifiedSequenceDataset(Dataset):
    """
    ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œæ”¯æŒä¸¤ç§æ“ä½œæ¨¡å¼ï¼š
    1. 'generate': å®æ—¶ç”Ÿæˆæ•°æ®å¹¶è‡ªåŠ¨å­˜æ¡£ã€‚
    2. 'load': ä»ç£ç›˜åŠ è½½é¢„å…ˆå­˜æ¡£çš„æ•°æ®ã€‚
    """
    def __init__(self, config: dict, split: str):
        self.config = config
        self.split = split
        self.mode = config['data_pipeline']['mode'] # 'generate' or 'load'
        
        data_config = config['data_pipeline']
        self.h5_archive_dir = os.path.join(data_config['h5_archive_path'], split)
        os.makedirs(self.h5_archive_dir, exist_ok=True)

        if self.mode == 'generate':
            self.generator = GenerativeBackend(config, split)
            self.num_sequences_per_epoch = config['training' if split == 'train' else 'evaluation']['num_long_sequences_per_epoch']
            # print(f"UnifiedDataset ({split}) in 'generate' mode. Will generate {self.num_sequences_per_epoch} sequences per epoch.")
        
        elif self.mode == 'load':
            self.file_list = sorted(glob.glob(os.path.join(self.h5_archive_dir, '*.h5')))
            if not self.file_list:
                raise FileNotFoundError(f"No .h5 files found in {self.h5_archive_dir}. Please run in 'generate' mode first.")
            self.num_sequences_per_epoch = len(self.file_list)
            # print(f"UnifiedDataset ({split}) in 'load' mode. Found {self.num_sequences_per_epoch} pre-generated files.")
        
        else:
            raise ValueError(f"Unknown mode in data_pipeline: {self.mode}")

    def __len__(self):
        return self.num_sequences_per_epoch

    def __getitem__(self, idx):
        if self.mode == 'generate':
            # 1. å®æ—¶ç”Ÿæˆä¸€ä¸ªé•¿åºåˆ—
            features, labels = self.generator._generate_one_long_sequence()
            
            # 2. è‡ªåŠ¨å­˜æ¡£åˆ°ç£ç›˜ - é˜²é‡åæœºåˆ¶
            import random
            timestamp = int(time.time() * 1000)
            base_filename = f'sequence_{timestamp}_{idx:05d}'
            
            # é˜²é‡åï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ éšæœºåç¼€
            attempt = 0
            while attempt < 10:  # æœ€å¤šå°è¯•10æ¬¡
                if attempt == 0:
                    filename = f'{base_filename}.h5'
                else:
                    random_suffix = random.randint(1000, 9999)
                    filename = f'{base_filename}_{random_suffix}.h5'
                
                file_path = os.path.join(self.h5_archive_dir, filename)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if not os.path.exists(file_path):
                    break
                attempt += 1
            else:
                # å¦‚æœ10æ¬¡éƒ½å¤±è´¥ï¼Œä½¿ç”¨UUIDç¡®ä¿å”¯ä¸€æ€§
                import uuid
                unique_id = str(uuid.uuid4())[:8]
                filename = f'{base_filename}_{unique_id}.h5'
                file_path = os.path.join(self.h5_archive_dir, filename)
            
            try:
                with h5py.File(file_path, 'w') as hf:
                    hf.create_dataset('features', data=features)
                    hf.create_dataset('labels', data=labels)
                # print(f"ğŸ’¾ Archived sequence to: {filename} (shape: {features.shape})")
            except Exception as e:
                print(f"Warning: Failed to save archive file {file_path}. Error: {e}")

            # 3. è½¬æ¢ä¸ºTensorå¹¶è¿”å›
            return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)

        elif self.mode == 'load':
            # ç›´æ¥ä»ç£ç›˜è¯»å–æ–‡ä»¶
            file_path = self.file_list[idx]
            with h5py.File(file_path, 'r') as hf:
                features = hf['features'][:]
                labels = hf['labels'][:]
            
            filename = os.path.basename(file_path)
            # print(f"ğŸ“‚ Loaded sequence from: {filename} (shape: {features.shape})")
            return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)

def create_unified_dataloaders(config):
    """æ ¹æ®é…ç½®åˆ›å»ºç»Ÿä¸€çš„Dataloader"""
    train_dataset = UnifiedSequenceDataset(config, split='train')
    val_dataset = UnifiedSequenceDataset(config, split='val')
    
    # ### BEGIN BUGFIX 4: DATALOADER CONFIG ###
    # å¯¹äºæœ‰çŠ¶æ€TBPTTï¼Œæ°¸è¿œä¸è¦shuffleï¼Œå¹¶ä¸”ä½¿ç”¨å•è¿›ç¨‹åŠ è½½ä»¥ä¿è¯é¡ºåºå’Œç”Ÿæˆå®‰å…¨
    shuffle = False
    num_workers = 0
    # ### END BUGFIX 4 ###
    
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=shuffle, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers)

    print(f"ğŸ“Š Created Unified Dataloaders in '{config['data_pipeline']['mode']}' mode")
    print(f"  - Train: {len(train_loader)} sequences per epoch")
    print(f"  - Val:   {len(val_loader)} sequences per epoch")
    print(f"  - Shuffle: {shuffle}, Workers: {num_workers} (Fixed for TBPTT)")

    return train_loader, val_loader