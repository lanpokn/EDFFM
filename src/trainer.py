import torch
import torch.nn as nn
from tqdm import tqdm
import os
import glob

class Trainer:
    def __init__(self, model, train_loader, val_loader, config, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['training']['learning_rate']
        )
        # å»ºè®®ä½¿ç”¨BCEWithLogitsLossä»¥è·å¾—æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§
        self.criterion = nn.BCEWithLogitsLoss()
        self.epochs = self.config['training']['epochs']
        self.checkpoint_dir = self.config['training']['checkpoint_dir']
        
        # TBPTT parameters
        self.chunk_size = config['training']['chunk_size']
        # print(f"Trainer initialized with TBPTT chunk_size: {self.chunk_size}")
        
        # ç”¨äºæ–­ç‚¹ç»­è®­çš„çŠ¶æ€å˜é‡
        self.start_epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')
        self.current_epoch = 0

        # Checkpointä¿å­˜é¢‘ç‡ (åŸºäºå…¨å±€æ­¥æ•°)
        self.validate_every_n_steps = config['training'].get('validate_every_n_steps', 500)
        self.save_every_n_steps = config['training'].get('save_every_n_steps', 1000)
        
        # print(f"ğŸ’¾ Checkpoint schedule: validate every {self.validate_every_n_steps} steps, save every {self.save_every_n_steps} steps")
        
        os.makedirs(self.checkpoint_dir, exist_ok=True)

    def train_one_epoch(self):
        self.model.train()
        
        # å¤–å¾ªç¯: éå†é•¿åºåˆ—
        outer_loop_desc = f"Epoch {self.current_epoch+1}/{self.epochs}"
        for long_features, long_labels in tqdm(self.train_loader, desc=outer_loop_desc):
            # ### BEGIN BUGFIX 1: STATE LEAKAGE ###
            self.model.reset_hidden_state()
            # ### END BUGFIX 1 ###

            long_features = long_features.squeeze(0).to(self.device)
            long_labels = long_labels.squeeze(0).to(self.device)

            # å†…å¾ªç¯: TBPTTåˆ‡å—
            for i in range(0, long_features.shape[0], self.chunk_size):
                chunk_features = long_features[i : i + self.chunk_size]
                chunk_labels = long_labels[i : i + self.chunk_size]
                if chunk_features.shape[0] < 1: # è·³è¿‡ç©ºå—
                    continue
                
                # æ³¨æ„ï¼šä¸å†è·³è¿‡æœ€åä¸€ä¸ªä¸å®Œæ•´çš„å—ï¼Œè®©æ¨¡å‹ä¹Ÿå­¦ä¹ å®ƒ
                # if chunk_features.shape[0] != self.chunk_size:
                #     continue
                    
                chunk_features = chunk_features.unsqueeze(0)
                self.optimizer.zero_grad()
                
                # å‡è®¾æ¨¡å‹è¾“å‡ºlogits (BCEWithLogitsLoss)
                predictions = self.model(chunk_features)
                
                # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…æŸå¤±å‡½æ•°
                # predictions: [1, L, 1] -> [L]
                # chunk_labels: [L]
                loss = self.criterion(predictions.squeeze(), chunk_labels.float())
                loss.backward()
                
                # ### BEGIN RISK MITIGATION 3: GRADIENT CLIPPING ###
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                # ### END RISK MITIGATION 3 ###
                
                self.optimizer.step()

                # å…¨å±€æ­¥æ•°æ˜¯å”¯ä¸€çš„æ—¶é—´æˆ³
                self.global_step += 1

                # å‘¨æœŸæ€§éªŒè¯å’Œä¿å­˜
                if self.global_step > 0 and self.global_step % self.validate_every_n_steps == 0:
                    val_loss = self.validate_one_epoch()
                    print(f"\nğŸ“Š Step {self.global_step} | Val Loss: {val_loss:.4f} | Best: {self.best_val_loss:.4f}")
                    if val_loss < self.best_val_loss:
                        self.best_val_loss = val_loss
                        self._save_checkpoint(is_best=True)
                
                if self.global_step > 0 and self.global_step % self.save_every_n_steps == 0:
                    self._save_checkpoint(is_best=False)
    
    def validate_one_epoch(self):
        self.model.eval()
        total_loss = 0
        total_chunks_processed = 0
        
        with torch.no_grad():
            # å¤–å¾ªç¯ï¼šéå†æ‰€æœ‰é•¿åºåˆ—
            for long_features, long_labels in tqdm(self.val_loader, desc="Validating"):
                # ### BEGIN BUGFIX 1: STATE LEAKAGE ###
                self.model.reset_hidden_state()
                # ### END BUGFIX 1 ###

                # DataLoaderè¿”å›(1, L, D)ï¼Œéœ€è¦è§£åŒ…
                long_features = long_features.squeeze(0).to(self.device)
                long_labels = long_labels.squeeze(0).to(self.device)

                # å†…å¾ªç¯ï¼šåœ¨å½“å‰é•¿åºåˆ—ä¸Šè¿›è¡Œåˆ‡å—éªŒè¯
                for i in range(0, long_features.shape[0], self.chunk_size):
                    
                    # 1. åˆ‡åˆ†å‡ºå›ºå®šé•¿åº¦çš„å—
                    chunk_features = long_features[i : i + self.chunk_size]
                    chunk_labels = long_labels[i : i + self.chunk_size]

                    if chunk_features.shape[0] < 1:
                        continue
                    
                    # æ³¨æ„ï¼šéªŒè¯æ—¶ä¹Ÿä¸å†è·³è¿‡æœ€åä¸€ä¸ªå—
                    # if chunk_features.shape[0] != self.chunk_size:
                    #     continue

                    chunk_features = chunk_features.unsqueeze(0)
                    predictions = self.model(chunk_features)
                    
                    loss = self.criterion(predictions.squeeze(), chunk_labels.float())
                    
                    # æŒ‰å—çš„é•¿åº¦åŠ æƒæŸå¤±ï¼Œæ›´å…¬å¹³
                    total_loss += loss.item() * len(chunk_features)
                    total_chunks_processed += len(chunk_features)
                    
        return total_loss / total_chunks_processed if total_chunks_processed > 0 else 0

    def _save_checkpoint(self, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­"""
        state = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
        }
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹ï¼Œæ–‡ä»¶ååŒ…å«å…¨å±€æ­¥æ•°ï¼Œä¾¿äºæ’åº
        filename = os.path.join(self.checkpoint_dir, f'ckpt_step_{self.global_step:08d}.pth')
        torch.save(state, filename)
        # print(f"\nğŸ’¾ Checkpoint saved to {os.path.basename(filename)}")

        if is_best:
            best_filename = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(state, best_filename)
            # print(f"ğŸ† Best model updated and saved")

    def _load_checkpoint(self):
        """åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹"""
        # å¯»æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        checkpoints = glob.glob(os.path.join(self.checkpoint_dir, 'ckpt_step_*.pth'))
        if not checkpoints:
            # print("INFO: No checkpoint found, starting from scratch.")
            return

        latest_checkpoint_path = max(checkpoints, key=os.path.getctime)
        print(f"ğŸ”„ Resuming from checkpoint: {os.path.basename(latest_checkpoint_path)}")
        
        checkpoint = torch.load(latest_checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.start_epoch = checkpoint['epoch']
        # å…³é”®ï¼šå¦‚æœä»epochä¸­é—´æ¢å¤ï¼Œéœ€è¦ä¿è¯global_stepä¹Ÿæ¢å¤
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        print(f"   â†’ Epoch {self.start_epoch + 1}, Step {self.global_step}")

    def train(self):
        print("ğŸš€ Starting TBPTT training...")
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        self._load_checkpoint()
        
        for epoch in range(self.start_epoch, self.epochs):
            self.current_epoch = epoch
            print(f"\nEpoch {epoch + 1}/{self.epochs} (step {self.global_step})")
            self.train_one_epoch() # éªŒè¯å’Œä¿å­˜é€»è¾‘å·²ç§»å…¥æ­¤å‡½æ•°
            
            # Epochç»“æŸæ—¶ä¹Ÿä¿å­˜ä¸€æ¬¡ï¼Œä»¥é˜²ä¸‡ä¸€
            self._save_checkpoint(is_best=False)
            
        print("ğŸ Training completed.")