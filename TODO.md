run_project.sh is important, can let you know the environment you need:
source /home/lanpoknlanpokn/miniconda3/bin/activate event_flare

source /home/lanpoknlanpokn/miniconda3/bin/activate event_flare && python main.py --config configs/config.yaml --debug


禁用val，或者val从其他地方拿，不然本来也是仿真的东西
保存的h5重名问题
flare的频率可以再翻倍
存的是处理过的x,y,dt,p和label而不是原始的xytp,这点要注意，后续想补feature extractor就不容易了

面向val_loss来调整学习率？

inference怎么这么慢？有余力去思考实时推理.

目前去炫光效果并不好，但也许只是训练还没有收敛？

重新引入PFD，让gemeni想该怎么做

现在的inference并不对，数据集也不是原始数据，肯定不够好，都需要改动。不过我现在最需要的恐怕不是沉浸在这些无聊的工程细节上，而是想好一种最基本的
数据生成应该有各种不同的mode



后续想办法认真分析如何同时仿真成对光源事件，先跟gemini唠清楚，喂给关键代码，有了计划书再让claude干活，关键就是光源无glsl以及对齐。

为了数据集的多样性合理性，后续bg_event要有仿真数据（resize成640x480），炫光要有其他来源仿真数据或者真实数据。为此，bg——event读取需要随机从DSEC或者本地读取。


基于物理的混合模型，关键是不再完全合并，而是合并时，每个像素位置都有一个概率记作A，炫光事件（或者光源事件）在这个像素上在输入时有A的概率被保留，同一个像素位置的背景事件则会有1-A的概率被保留。实际上我们知道，A背后是由w_{\text{flare}}(t) &= \frac{Y_{\text{flare}}(t)}{Y_{\text{bg}}(t) + Y_{\text{flare}}(t)}判断的。现在问题在于，这个A是逐像素计算的，因此我们无法简单的对原始的事件先概率滤除后再加起来，那肯定是不对的，因为诸如只有背景没有炫光的区域，A应该是0，而（强）光源区域A应该近似为1。 那么如何才能做好这个模型呢,这个A的估计是巨大的难题，尤其是细想的话A还是时变的？

我有一个初步想法1，首先把事件数据根据x,y的位置，分成M*N个子事件数据，然后统计各个子事件数据的总数量，作为它们Y的近似（不过为了模拟炫光的高光强，炫光或者光源事件可以取事件总数乘以k)，如此便可以计算出各个像素的A。之后根据这M*N个子时间数据以及对应的A（为了真实性模拟，在上述A的基础上可以有一定波动），把这些子事件数据概率去除（再有点小的时间扰动）后，再加起来，就可以得到合并事件的M*N个子事件，这些子事件最后再合并并排序，就可以得到最终事件。

想法1本质上分成A估计，与基于A的合成上，A的估计还可以我在炫光仿真时存一下炫光平均强度图，然后假设背景光强恒定。 基于A的合成，还可以在原始事件上滤波，读取一个事件，根据事件的x,y属性去拿对应的A然后给一个概率滤除，这样就没有那个M*N个子事件序列的合并再排序，而是直接最后排序一下就可以了。

还要有单纯视频输入，生成事件数据的简单版本。这个版本下，正好调整k1的值，方便我观察不同参数对仿真的影响

也许严格的按照阈值去加，时间复杂度可以控住！

source /home/lanpoknlanpokn/miniconda3/bin/activate event_flare && python main.py --step 2 --debug有bug，交给gemeni改吧

A现在欠缺随机性，再随机一点（但不能超过1，会好一点）而且用事件数量估计A实在是太没道理了，炫光和光源一个强度，肯定不对，就改这个！
没这个必要！先把框架打通，后续有的是改进的地方，第一个工作不要追求完美，先跑通，然后再思考各种可能把它变好的途径。

其他A的优化确实可以停一停，但是A的合理随机化方式还是值得思考一下的，以及直接读一个A(x,y)到h5中，用基于光学的方法估计A(x,y)
或者这个可以先放着，后续谁拿去水论文去吧


后续：让它实际分析一下平均事件的间隔再给jitter

存在奇怪的bug！存在1kb的文件！需要找到其具体原因,很可能目前没这个问题了。
那个训练集偏向可能太细了，先别考虑了，学术不用考虑这个